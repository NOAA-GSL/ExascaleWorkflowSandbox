hercules-env: &hercules-env
  environment:
    - "source /apps/systools/hercules/etc/HPC2-lmod.sh"
    - "module purge"
    - "module use /apps/contrib/spack-stack/spack-stack-1.8.0/envs/ue-intel-2021.9.0/install/modulefiles/Core"
    - "module load stack-intel/2021.9.0"
    - "module load stack-intel-oneapi-mpi/2021.9.0"
    - "module load stack-python/3.11.7"
    - "module load jedi-fv3-env"
    - "export CHILTEPIN_MPIF90=$MPIF90"

hera-env: &hera-env
  environment:
    - "source /etc/profile.d/modules.sh"
    - "module purge"
    - "module use /contrib/spack-stack/spack-stack-1.8.0/envs/ue-intel-2021.5.0/install/modulefiles/Core"
    - "module load stack-intel/2021.5.0"
    - "module load stack-intel-oneapi-mpi/2021.5.1"
    - "module load stack-python/3.11.7"
    - "module load jedi-fv3-env"
    - "export CHILTEPIN_MPIF90=$MPIF90"

ursa-env: &ursa-env
  environment:
    - 'module help > /dev/null 2>&1 || source $(bash -l -c "echo $MODULESHOME")/init/profile'
    - "module purge"
    - "module use /contrib/spack-stack/spack-stack-1.9.3/envs/ue-oneapi-2024.2.1/install/modulefiles/Core"
    - "module load stack-oneapi/2024.2.1"
    - "module load stack-python/3.11.7"
    - "module load stack-intel-oneapi-mpi/2021.13"
    - "export CHILTEPIN_MPIF90=$MPIF90"

docker-env: &docker-env
  environment:
    - "source /usr/lmod/lmod/init/bash"
    - "module use /opt/spack-stack/envs/unified-env/install/modulefiles/Core"
    - "module load stack-gcc"
    - "module load stack-openmpi"
    - "module load stack-python"
    - "module load jedi-fv3-env"
    - "module unload py-attrs"
    - "export CHILTEPIN_MPIF90=$MPIF90"

hercules:
  resources:
    mpi:
      mpi: True
      max_mpi_apps: 2
      mpi_launcher: "srun"
      provider: "slurm"
      cores_per_node: 80
      nodes_per_block: 3
      exclusive: True
      partition: "hercules"
      account: "gsd-hpcs"
      walltime: "1:00:00"
      <<: *hercules-env
    service:
      mpi: False
      provider: "slurm"
      cores_per_node: 1
      nodes_per_block: 1
      exclusive: False
      partition: "service"
      account: "gsd-hpcs"
      walltime: "1:00:00"
      <<: *hercules-env
    compute:
      mpi: False
      provider: "slurm"
      cores_per_node: 80
      nodes_per_block: 1
      exclusive: True
      partition: "hercules"
      account: "gsd-hpcs"
      walltime: "1:00:00"
      <<: *hercules-env
    gc-mpi:
      endpoint: "{{ mpi_endpoint_id }}"
      mpi: True
      max_mpi_apps: 2
      mpi_launcher: "srun"
      provider: "slurm"
      cores_per_node: 80
      nodes_per_block: 3
      exclusive: True
      partition: "hercules"
      account: "gsd-hpcs"
      walltime: "1:00:00"
      <<: *hercules-env
    gc-service:
      endpoint: "{{ service_endpoint_id }}"
      mpi: False
      provider: "slurm"
      cores_per_node: 1
      nodes_per_block: 1
      exclusive: False
      partition: "service"
      account: "gsd-hpcs"
      walltime: "1:00:00"
      <<: *hercules-env
    gc-compute:
      endpoint: "{{ compute_endpoint_id }}"
      mpi: False
      provider: "slurm"
      cores_per_node: 80
      nodes_per_block: 1
      exclusive: True
      partition: "hercules"
      account: "gsd-hpcs"
      walltime: "1:00:00"
      <<: *hercules-env

hera:
  resources:
    mpi:
      mpi: True
      max_mpi_apps: 2
      mpi_launcher: "srun"
      provider: "slurm"
      cores_per_node: 40
      nodes_per_block: 3
      exclusive: True
      partition: "hera"
      account: "gsd-hpcs"
      walltime: "1:00:00"
      <<: *hera-env
    service:
      mpi: False
      provider: "slurm"
      cores_per_node: 1
      nodes_per_block: 1
      exclusive: False
      partition: "service"
      account: "gsd-hpcs"
      walltime: "1:00:00"
      <<: *hera-env
    compute:
      mpi: False
      provider: "slurm"
      cores_per_node: 40
      nodes_per_block: 1
      exclusive: True
      partition: "hera"
      account: "gsd-hpcs"
      walltime: "1:00:00"
      <<: *hera-env
    gc-mpi:
      endpoint: "{{ mpi_endpoint_id }}"
      mpi: True
      max_mpi_apps: 2
      mpi_launcher: "srun"
      provider: "slurm"      
      cores_per_node: 40
      nodes_per_block: 3
      exclusive: True
      partition: "hera"
      account: "gsd-hpcs"
      walltime: "1:00:00"
      <<: *hera-env
    gc-service:
      endpoint: "{{ service_endpoint_id }}"
      mpi: False
      provider: "slurm"
      cores_per_node: 1
      nodes_per_block: 1
      exclusive: False
      partition: "service"
      account: "gsd-hpcs"
      walltime: "1:00:00"
      <<: *hera-env
    gc-compute:
      endpoint: "{{ compute_endpoint_id }}"
      mpi: False
      provider: "slurm"
      cores_per_node: 40
      nodes_per_block: 1
      exclusive: True
      partition: "hera"
      account: "gsd-hpcs"
      walltime: "1:00:00"
      <<: *hera-env

ursa:
  resources:
    mpi:
      mpi: True
      max_mpi_apps: 2
      mpi_launcher: "srun"
      provider: "slurm"
      cores_per_node: 192
      nodes_per_block: 3
      exclusive: True
      partition: "u1-compute"
      queue: "debug"
      account: "gsd-hpcs"
      walltime: "00:10:00"
      <<: *ursa-env
    service:
      mpi: False
      provider: "slurm"
      cores_per_node: 1
      nodes_per_block: 1
      exclusive: False
      partition: "u1-service"
      queue: "debug"
      account: "gsd-hpcs"
      walltime: "00:10:00"
      <<: *ursa-env
    compute:
      mpi: False
      provider: "slurm"
      cores_per_node: 40
      nodes_per_block: 1
      exclusive: True
      partition: "u1-compute"
      queue: "debug"
      account: "gsd-hpcs"
      walltime: "00:10:00"
      <<: *ursa-env
    gc-mpi:
      endpoint: "{{ mpi_endpoint_id }}"
      mpi: True
      max_mpi_apps: 2
      mpi_launcher: "srun"
      provider: "slurm"
      cores_per_node: 40
      nodes_per_block: 3
      exclusive: True
      partition: "u1-compute"
      queue: "debug"
      account: "gsd-hpcs"
      walltime: "00:10:00"
      <<: *ursa-env
    gc-service:
      endpoint: "{{ service_endpoint_id }}"
      mpi: False
      provider: "slurm"
      cores_per_node: 1
      nodes_per_block: 1
      exclusive: False
      partition: "u1-service"
      queue: "debug"
      account: "gsd-hpcs"
      walltime: "00:10:00"
      <<: *ursa-env
    gc-compute:
      endpoint: "{{ compute_endpoint_id }}"
      mpi: False
      provider: "slurm"
      cores_per_node: 40
      nodes_per_block: 1
      exclusive: True
      partition: "u1-compute"
      queue: "debug"
      account: "gsd-hpcs"
      walltime: "00:10:00"
      <<: *ursa-env

docker:
  resources:
    mpi:
      mpi: True
      max_mpi_apps: 2
      mpi_launcher: "srun"
      provider: "slurm"
      cores_per_node: 8
      nodes_per_block: 3
      exclusive: True
      partition: "slurmpar"
      account: ""
      walltime: "1:00:00"
      <<: *docker-env
    service:
      mpi: False
      provider: "slurm"
      cores_per_node: 1
      nodes_per_block: 1
      exclusive: False
      partition: "slurmpar"
      account: ""
      walltime: "1:00:00"
      <<: *docker-env
    compute:
      mpi: False
      provider: "slurm"
      cores_per_node: 8
      nodes_per_block: 1
      exclusive: True
      partition: "slurmpar"
      account: ""
      walltime: "1:00:00"
      <<: *docker-env
    gc-mpi:
      endpoint: "{{ mpi_endpoint_id }}"
      mpi: True
      max_mpi_apps: 2
      mpi_launcher: "srun"
      provider: "slurm"
      cores_per_node: 8
      nodes_per_block: 3
      exclusive: True
      partition: "slurmpar"
      account: ""
      walltime: "1:00:00"
      <<: *docker-env
    gc-service:
      endpoint: "{{ service_endpoint_id }}"
      mpi: False
      provider: "slurm"
      cores_per_node: 1
      nodes_per_block: 1
      exclusive: False
      partition: "slurmpar"
      account: ""
      walltime: "1:00:00"
      <<: *docker-env
    gc-compute:
      endpoint: "{{ compute_endpoint_id }}"
      mpi: False
      provider: "slurm"
      cores_per_node: 8
      nodes_per_block: 1
      exclusive: True
      partition: "slurmpar"
      account: ""
      walltime: "1:00:00"
      <<: *docker-env
